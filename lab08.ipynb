{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "consolidated-difference",
   "metadata": {},
   "source": [
    "# CSC311 Lab 8: Naive Bayes for Classifying Movie Reviews\n",
    "\n",
    "In this lab, we will build a naive bayes model to classify positive vs\n",
    "negative movie reviews.\n",
    "\n",
    "By the end of this lab , you will be able to:\n",
    "\n",
    "1. Use the \"bag of word\" representation of text.\n",
    "2. Build naive bayes classifiers to solve classification problems.\n",
    "3. Analyze MLE vs Bayesian parameter estimation methods.\n",
    "4. Use a naive bayes classifier to make predictions.\n",
    "\n",
    "Please work in groups of 1-2 during the lab.\n",
    "\n",
    "\n",
    "## Submission\n",
    "\n",
    "Submit a PDF document called `lab08.pdf` on Markus. \n",
    "You may also, optionally, submit your lab09.ipynb file. Your PDF file will be graded, but\n",
    "we will ask the TAs to check your ipynb file in case any solutions are cut off. Annotations will be \n",
    "made in the PDF file.\n",
    "Your PDF file should contain the following:\n",
    "\n",
    "- Part 1. your creation of the `vocab` list and report of the size of the vocabulary (1 point)\n",
    "- Part 1. your implementation of the `make_bow` function (1 point)\n",
    "- Part 2. your implementation of the `naive_bayes_mle` function (2 points)\n",
    "- Part 2. your implementation of the `naive_bayes_map` function (2 points)\n",
    "- Part 3. your implementation of the `make_prediction` function (3 points)\n",
    "- Part 3. your training/validation accuracy of a logistic regression model trained on this data (1 point)\n",
    "\n",
    "You may produce a PDF document by exploring the Colab document, but be careful to check\n",
    "that the required code and output is not cut off.\n",
    "This method is preferred, since we would be able to more easily help point out issues.\n",
    "\n",
    "Alternatively, you may create a PDF document that contain the parts that are graded.\n",
    "However, the feedback we are able to provide may be more limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-princess",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "Data is a variation of the one from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews,\n",
    "pre-processed so that only 1000 words are in the training/test set.\n",
    "\n",
    "\n",
    "## Part 1 Data\n",
    "\n",
    "Start by running these two lines of code to download the data on to Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download tutorial data files.\n",
    "!wget https://www.cs.toronto.edu/~lczhang/311/lab09/trainvalid.csv\n",
    "!wget https://www.cs.toronto.edu/~lczhang/311/lab09/test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-excellence",
   "metadata": {},
   "source": [
    "As always, we start by understanding what our data looks like. Notice that the\n",
    "test set has been set aside for us. Both the training and test set files follow\n",
    "the same format, where each line in the csv file contains the review text and\n",
    "the string label \"positive\" or \"negative\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "trainfile = \"trainvalid.csv\"\n",
    "testfile = \"test.csv\"\n",
    "\n",
    "# Training/Validation set\n",
    "data = csv.reader(open(trainfile))\n",
    "for i, line in enumerate(data):\n",
    "    print(line)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set (separated so that we will all have the same test set)\n",
    "data = csv.reader(open(testfile))\n",
    "for i, line in enumerate(data):\n",
    "    print(line)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-syndicate",
   "metadata": {},
   "source": [
    "**Task**: How many positive reviews are in `trainvalid.csv`?  Negative reviews? What about in `test.csv`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-spiritual",
   "metadata": {},
   "source": [
    "**Graded Task**: How many unique words are in the entire training/validation set?\n",
    "This will be the size of our **vocabulary** when we convert to a bag-of-word representation.\n",
    "To compute this value, produce a list `vocab` that contains a list of unique words \n",
    "in `trainvalid.csv`.\n",
    "\n",
    "Splitting sentences into words is typically not a trivial operation. For this lab, punctuations\n",
    "and other special symbols have already been removed from the reviews. So, using python's \n",
    "`string.split()` method is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = None # TODO\n",
    "\n",
    "print(\"Vocabulary Size: \", len(vocab)) # Please include the output of this statement in your submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-vacuum",
   "metadata": {},
   "source": [
    "**Graded Task**: Complete the function `make_bow`, which takes a list of `(review, label)` pairs\n",
    "and a list of words in the vocabulary, and produces a data matrix consisting of bag-of-word features,\n",
    "along with a vector of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow(data, vocab):\n",
    "    \"\"\"\n",
    "    Produce the bag-of-word representation of the data, along with a vector\n",
    "    of labels. You *may* use loops to iterate over `data`. However, your code\n",
    "    should not take more than O(len(data) * len(vocab)) to run.\n",
    "\n",
    "    Parameters:\n",
    "        `data`: a list of `(review, label)` pairs, like those produced from\n",
    "                `list(csv.reader(open(\"trainvalid.csv\")))`\n",
    "        `vocab`: a list consisting of all unique words in the vocabulary\n",
    "\n",
    "    Returns:\n",
    "        `X`: A data matrix of bag-of-word features. This data matrix should be\n",
    "             a numpy array with shape [len(data), len(vocab)].\n",
    "             Moreover, `X[i,j] == 1` if the review in `data[i]` contains the\n",
    "             word `vocab[j]`, and `X[i,j] == 0` otherwise.\n",
    "        `t`: A numpy array of shape [len(data)], with `t[i] == 1` if \n",
    "             `data[i]` is a positive review, and `t[i] == 0` otherwise.\n",
    "    \"\"\"\n",
    "    X = np.zeros([len(data), len(vocab)])\n",
    "    t = np.zeros([len(data)])\n",
    "\n",
    "    # TODO: fill in the appropriate values of X and t\n",
    "\n",
    "    return X, t\n",
    "\n",
    "# Separate data into training/validation, then call `make_bow` to produce\n",
    "# the bag-of-word features\n",
    "import random\n",
    "random.seed(42)\n",
    "data = list(csv.reader(open(trainfile)))\n",
    "random.shuffle(data)\n",
    "X_train, t_train = make_bow(data[:10000], vocab)\n",
    "X_valid, t_valid = make_bow(data[10000:], vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-toyota",
   "metadata": {},
   "source": [
    "**Task**: It is a good idea to understand the distribution of relative word occurrences.\n",
    "Produce the figure below, which shows how often each word occurs in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce the mapping of words to count\n",
    "vocab_count_mapping = list(zip(vocab, np.sum(X_train, axis=0)))\n",
    "vocab_count_mapping = sorted(vocab_count_mapping, key=lambda e: e[1], reverse=True)\n",
    "for word, cnt in vocab_count_mapping:\n",
    "    print(word, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-crime",
   "metadata": {},
   "source": [
    "If we plot these occurrences, we see that the more common words tend to occur much more frequently\n",
    "compared to the less common words. This type of distribution occurs quite often in\n",
    "typical data sets, where there are a **long tail** of words that occur infrequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([cnt for word, cnt in vocab_count_mapping])\n",
    "plt.title(\"Distribution of words in the training data\")\n",
    "plt.xlabel(\"Word ranking (1=most common, 1000=least common)\")\n",
    "plt.ylabel(\"Number of occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-wallace",
   "metadata": {},
   "source": [
    "## Part 2. Naive Bayes Model\n",
    "\n",
    "In this section, we will build a naive bayes model to predict whether a movie review\n",
    "is positive or negative. The naive bayes model is a generative model and a probabilistic classifier,\n",
    "where we will make the **conditional independence assumption**:\n",
    "$$p(c, x_1, \\ldots, x_D) = p(c) p(x_1 | c) \\cdots p(x_D | c)$$\n",
    "\n",
    "Where $c$ is the label class  (positive or negative), and each $x_j \\in {0, 1}$ represents whether\n",
    "word $j$ appears in the review. We will fit parameters $\\pi$ and $\\theta_{jc}$, with:\n",
    "\n",
    "- $p(c=1) = \\pi$, representing the probability of positive class\n",
    "- $p(x_j = 1 | c) = \\theta_{jc}$, representing the probability of word $j$ appearing in a review with class $c$.\n",
    "\n",
    "**Graded Task**: Implement the function `naive_bayes_mle` that computes the MLE estimation of \n",
    "parameters. This function will produce the estimates of $\\pi$ and the $\\theta_{jc}$s that optimizes\n",
    "the log likelihood of the training data:\n",
    "\n",
    "$$\\ell(\\theta) = \\sum_{i=1}^N \\log p(c^{(i)}, {\\bf x}^{(i)})$$\n",
    "\n",
    "Like mentioned in class, this formula sounds scary, but the estimates of $\\pi$ and $\\theta_{jc}$ turns out\n",
    "to require only basic counting operations and division.\n",
    "As such, we can implement this function in an entirely vectorized way, **without using loops**.\n",
    "Please do not use any loops in your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_mle(X, t):\n",
    "    \"\"\"\n",
    "    Compute the parameters $pi$ and $theta_{jc}$ that maximizes the log-likelihood\n",
    "    of the provided data (X, t). \n",
    "\n",
    "    **Your solution should be vectorized, and contain no loops**\n",
    "\n",
    "    Parameters:\n",
    "        `X` - a matrix of bag-of-word features of shape [N, V],\n",
    "              where N is the number of data points and V is the vocabulary size.\n",
    "              X[i,j] should be either 0 or 1. Produced by the make_bow() function.\n",
    "        `t` - a vector of class labels of shape [N], with t[i] being either 0 or 1.\n",
    "              Produced by the make_bow() function.\n",
    "\n",
    "    Returns:\n",
    "        `pi` - a scalar; the MLE estimate of the parameter $\\pi = p(c = 1)$\n",
    "        `theta` - a matrix of shape [V, 2], where `theta[j, c]` corresponds to\n",
    "                  the MLE estimate of the parameter $\\theta_{jc} = p(x_j = 1 | c)$\n",
    "    \"\"\"\n",
    "    N, vocab_size = X.shape[0], X.shape[1]\n",
    "    pi = 0 # TODO\n",
    "    theta = np.zeros([vocab_size, 2]) # TODO\n",
    "\n",
    "    # these matrices may be useful (but what do they represent?)\n",
    "    X_positive = X[t == 1]\n",
    "    X_negative = X[t == 0]\n",
    "\n",
    "    # theta[:, 1] = None # TODO: you may uncomment this line if you'd like\n",
    "    # theta[:, 0] = None # TODO: you may uncomment this line if you'd like\n",
    "\n",
    "    return pi, theta\n",
    "\n",
    "pi_mle, theta_mle = naive_bayes_mle(X_train, t_train)\n",
    "\n",
    "print(theta_mle.shape) # should be (1000, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-entry",
   "metadata": {},
   "source": [
    "**Graded Task**: Implement the function `naive_bayes_map` that computes the MAP estimation of \n",
    "parameters. This function will produce the estimates of $\\pi$ and the $\\theta_{jc}$s that maximizes\n",
    "the posterior: $p(\\theta | \\mathcal{D})$, where $\\theta = {\\pi, \\theta_{jc}}$ consists of all\n",
    "of our parameters.\n",
    "\n",
    "We will use the beta distribution with $a=2$ and $b=2$ for all of our parameters.\n",
    "\n",
    "Once again, although these words might sound scary, the estimates of $\\pi$ and $\\theta_{jc}$ turns out\n",
    "to require only basic counting operations and division---and not much more than the previous part!\n",
    "Again, we can implement this function in an entirely vectorized way, **without using loops**.\n",
    "Please do not use any loops in your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_map(X, t):\n",
    "    \"\"\"\n",
    "    Compute the parameters $pi$ and $theta_{jc}$ that maximizes the posterior\n",
    "    of the provided data (X, t). We will use the beta distribution with\n",
    "    $a=2$ and $b=2$ for all of our parameters.\n",
    "\n",
    "    **Your solution should be vectorized, and contain no loops**\n",
    "\n",
    "    Parameters:\n",
    "        `X` - a matrix of bag-of-word features of shape [N, V],\n",
    "              where N is the number of data points and V is the vocabulary size.\n",
    "              X[i,j] should be either 0 or 1. Produced by the make_bow() function.\n",
    "        `t` - a vector of class labels of shape [N], with t[i] being either 0 or 1.\n",
    "              Produced by the make_bow() function.\n",
    "\n",
    "    Returns:\n",
    "        `pi` - a scalar; the MAP estimate of the parameter $\\pi = p(c = 1)$\n",
    "        `theta` - a matrix of shape [V, 2], where `theta[j, c]` corresponds to\n",
    "                  the MAP estimate of the parameter $\\theta_{jc} = p(x_j = 1 | c)$\n",
    "    \"\"\"\n",
    "\n",
    "pi_map, theta_map = naive_bayes_map(X_train, t_train)\n",
    "\n",
    "print(theta_map.shape) # should be (1000, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-insured",
   "metadata": {},
   "source": [
    "## Part 3. Making predictions\n",
    "\n",
    "**Graded Task**: Complete the function `make_prediction` which uses our estimated parameters $\\pi$ and $\\theta_{jc}$ to make predictions on our dataset.\n",
    "\n",
    "Note that computing products of many small numbers leads to underflow. Use the fact that:\n",
    "\n",
    "$$a_1 \\cdot a_2 \\cdots a_n = e^{log(a_1) + log(a_2) + \\cdots + log(a_n)} $$\n",
    "\n",
    "to avoid computing a product of small numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(X, pi, theta):\n",
    "    y = None\n",
    "\n",
    "\n",
    "    return y\n",
    "\n",
    "def accuracy(y, t):\n",
    "    return np.mean(y == t)\n",
    "\n",
    "\n",
    "y_map_train = make_prediction(X_train, pi_map, theta_map)\n",
    "y_map_valid = make_prediction(X_valid, pi_map, theta_map)\n",
    "\n",
    "print(\"MAP Train Acc:\", accuracy(y_map_train, t_train))\n",
    "print(\"MAP Valid Acc:\", accuracy(y_map_valid, t_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-contemporary",
   "metadata": {},
   "source": [
    "At this point, you might wonder if the accuracy is \"about right\", and you might\n",
    "ask your instructors/TAs about what the expected accuracy might be.\n",
    "But what can we do to verify our results in real applications? One strategy\n",
    "is to compare your model output with other known models.\n",
    "\n",
    "**Graded Task**: Use sklearn to build a logistic regression model.\n",
    "Report the training and validation accuracy.\n",
    "You should see that the Logistic Regression model performs a bit better\n",
    "than the Naive Bayes model. The Logistic Regression model does not make\n",
    "as many assumptions about the data distribution that the Naive Bayes does.\n",
    "(Think: what are these assumptions?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "train_acc = None\n",
    "val_acc = None\n",
    "\n",
    "print(\"LR Train Acc:\", train_acc)\n",
    "print(\"LR Valid Acc:\", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-globe",
   "metadata": {},
   "source": [
    "This task is also a reminder that the bag of word features we built\n",
    "are just *features*, and can be used with *any* model!\n",
    "\n",
    "**Task**: The below code produces a warning or an error. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mle_train = make_prediction(X_train, pi_mle, theta_mle)\n",
    "y_mle_valid = make_prediction(X_valid, pi_mle, theta_mle)\n",
    "print(\"MLE Train Acc:\", accuracy(y_mle_train, t_train))\n",
    "print(\"MLE Valid Acc:\", accuracy(y_mle_valid, t_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-pointer",
   "metadata": {},
   "source": [
    "**Task**: The function `predict` is written for you, and provides another way to\n",
    "interact with the Naive Bayes model. The function takes a review (as a string),\n",
    "and relevant information about the naive bayes model, and produce an estimate \n",
    "for that single string.\n",
    "\n",
    "Call `predict` several times, with short reviews that you write, to get a sense\n",
    "of how the model behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(review, vocab, pi, theta):\n",
    "    x = np.zeros([1, len(vocab)])\n",
    "    words = review.split()\n",
    "    for j, w in enumerate(vocab):\n",
    "        if w in words:\n",
    "            x[0, j] = 1\n",
    "    return make_prediction(x, pi, theta)[0]\n",
    "\n",
    "predict(\"movie was fun\", vocab, pi_map, theta_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-paint",
   "metadata": {},
   "source": [
    "**Task**: Explain why the MLE model fails to make predictions (or produces a warning) for a review\n",
    "with the word \"william\" in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(\"william\", vocab, pi_map, theta_map)) # why does this succeed\n",
    "print(predict(\"william\", vocab, pi_mle, theta_mle)) # ...and this produce an error?\n",
    "\n",
    "# Your explanation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-nevada",
   "metadata": {},
   "source": [
    "**Task**: Report the test accuracy on the naive bayes model with parameter estimation via MAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-grounds",
   "metadata": {},
   "source": [
    "## Part 4. Analyzing the MAP Parameters\n",
    "\n",
    "**Task**: Write code to print the following:\n",
    "\n",
    "- the 10 words whose *presence* most strongly predicts that the review is positive\n",
    "- the 10 words whose *absense* most strongly predicts that the review is positive\n",
    "- the 10 words whose *presence* most strongly predicts that the review is negative\n",
    "- the 10 words whose *absense* most strongly predicts that the review is negative\n",
    "\n",
    "Note that these require more than just sorting the $\\theta_{jc}$s! (Why?)\n",
    "\n",
    "```\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
